{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a77567b-73f2-4436-b469-65d0a4f61cf6",
   "metadata": {},
   "source": [
    "Implementing Dyna-Q and Dyna-Q+ algorithms in maze environment. In this maze environment, the goal is to reach the goal state (G) as fast as possible from the starting state (S). There are four actions – up, down, right, left – which take the agent deterministically from a state to the corresponding neighboring states, except when movement is blocked by a wall (denoted by grey) or the edge of the maze, in which case the agent remains where it is. The reward is +1 on reaching the goal state, 0 otherwise. On reaching the goal state G, the agent returns to the start state S to being a new episode. This is a discounted, episodic task with gama = 0.95 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d100786-dbbe-4e4e-ba6a-2b472ce7a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, jdc, shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "from agent import BaseAgent\n",
    "from maze_env import ShortcutMazeEnvironment\n",
    "\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c33a41b-a8e6-49b3-a115-0aa39552409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.rcParams.update({'figure.figsize': [8,5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e161d-bb40-4369-9e59-397211578354",
   "metadata": {},
   "source": [
    "Dyna-Q involves four basic steps:\n",
    "1- Action selection: given an observation, select an action to be performed (here, using the \n",
    "-greedy method).\n",
    "2- Direct RL: using the observed next state and reward, update the action values (here, using one-step tabular Q-learning).\n",
    "3- Model learning: using the observed next state and reward, update the model (here, updating a table as the environment is assumed to be deterministic).\n",
    "4- Planning: update the action values by generating \"n\" simulated experiences using certain starting states and actions (here, using the random-sample one-step tabular Q-planning method). This is also known as the 'Indirect RL' step. The process of choosing the state and action to simulate an experience with is known as 'search control'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9faf45e9-36ea-4ff2-b44e-9a7c18939c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQAgent(BaseAgent):\n",
    "\n",
    "    def agent_init(self, agent_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Args:\n",
    "            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n",
    "            {\n",
    "                num_states (int): The number of states,\n",
    "                num_actions (int): The number of actions,\n",
    "                epsilon (float): The parameter for epsilon-greedy exploration,\n",
    "                step_size (float): The step-size,\n",
    "                discount (float): The discount factor,\n",
    "                planning_steps (int): The number of planning steps per environmental interaction\n",
    "\n",
    "                random_seed (int): the seed for the RNG used in epsilon-greedy\n",
    "                planning_random_seed (int): the seed for the RNG used in the planner\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            self.num_states = agent_info[\"num_states\"]\n",
    "            self.num_actions = agent_info[\"num_actions\"]\n",
    "        except:\n",
    "            print(\"You need to pass both 'num_states' and 'num_actions' \\\n",
    "                   in agent_info to initialize the action-value table\")\n",
    "        self.gamma = agent_info.get(\"discount\", 0.95)\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        self.planning_steps = agent_info.get(\"planning_steps\", 10)\n",
    "\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get('random_seed', 42))\n",
    "        self.planning_rand_generator = np.random.RandomState(agent_info.get('planning_random_seed', 42))\n",
    "\n",
    "        # Next, we initialize the attributes required by the agent, e.g., q_values, model, etc.\n",
    "        # A simple way to implement the model is to have a dictionary of dictionaries, \n",
    "        #        mapping each state to a dictionary which maps actions to (reward, next state) tuples.\n",
    "        self.q_values = np.zeros((self.num_states, self.num_actions))\n",
    "        self.actions = list(range(self.num_actions))\n",
    "        self.past_action = -1\n",
    "        self.past_state = -1\n",
    "        self.model = {} # model is a dictionary of dictionaries, which maps states to actions to \n",
    "                        # (reward, next_state) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d19db1-d9e0-49b5-8de1-2dbaa2a14bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DynaQAgent\n",
    "\n",
    "def update_model(self, past_state, past_action, state, reward):\n",
    "    \"\"\"updates the model \n",
    "    \n",
    "    Args:\n",
    "        past_state       (int): s\n",
    "        past_action      (int): a\n",
    "        state            (int): s'\n",
    "        reward           (int): r\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    # Update the model with the (s,a,s',r) tuple (1~4 lines)\n",
    "    \n",
    "    if past_state not in self.model:\n",
    "        self.model[past_state] = {}\n",
    "    self.model[past_state][past_action] = (state, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a428dc6-631e-47ee-a261-8f4c0a6653b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \n",
      " {0: {2: (0, 1), 3: (1, 2)}, 2: {0: (1, 1)}}\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 0}\n",
    "test_agent = DynaQAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "test_agent.update_model(0,2,0,1)\n",
    "test_agent.update_model(2,0,1,1)\n",
    "test_agent.update_model(0,3,1,2)\n",
    "print(\"Model: \\n\", test_agent.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60155c8c-3b3b-4eee-b10d-85c32fd32b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DynaQAgent\n",
    "\n",
    "# [GRADED]\n",
    "\n",
    "def planning_step(self):\n",
    "    \"\"\"performs planning, i.e. indirect RL.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    for _ in range(self.planning_steps):\n",
    "        past_state = self.planning_rand_generator.choice(list(self.model.keys()))\n",
    "        past_action = self.planning_rand_generator.choice(list(self.model[past_state].keys()))\n",
    "        state, reward = self.model[past_state][past_action]\n",
    "        if state != -1:\n",
    "            self.q_values[past_state][past_action] += self.step_size * (reward + self.gamma * np.max(self.q_values[state]) - self.q_values[past_state][past_action])\n",
    "        else:\n",
    "            self.q_values[past_state][past_action] += self.step_size * (reward - self.q_values[past_state][past_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edacb952-09e6-44a7-a403-45dca498f8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \n",
      " {0: {2: (1, 1), 3: (0, 1), 1: (-1, 1)}, 2: {0: (1, 1)}}\n",
      "Action-value estimates: \n",
      " [[0.  0.1 0.  0.2]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.1 0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"planning_steps\": 4,\n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 5}\n",
    "test_agent = DynaQAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "test_agent.update_model(0,2,1,1)\n",
    "test_agent.update_model(2,0,1,1)\n",
    "test_agent.update_model(0,3,0,1)\n",
    "test_agent.update_model(0,1,-1,1)\n",
    "test_agent.planning_step()\n",
    "print(\"Model: \\n\", test_agent.model)\n",
    "print(\"Action-value estimates: \\n\", test_agent.q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5224f2fc-f026-468e-82f4-d85476d2686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DynaQAgent\n",
    "\n",
    "def argmax(self, q_values):\n",
    "    \"\"\"argmax with random tie-breaking\n",
    "    Args:\n",
    "        q_values (Numpy array): the array of action values\n",
    "    Returns:\n",
    "        action (int): an action with the highest value\n",
    "    \"\"\"\n",
    "    top = float(\"-inf\")\n",
    "    ties = []\n",
    "\n",
    "    for i in range(len(q_values)):\n",
    "        if q_values[i] > top:\n",
    "            top = q_values[i]\n",
    "            ties = []\n",
    "\n",
    "        if q_values[i] == top:\n",
    "            ties.append(i)\n",
    "\n",
    "    return self.rand_generator.choice(ties)\n",
    "\n",
    "def choose_action_egreedy(self, state):\n",
    "    \"\"\"returns an action using an epsilon-greedy policy w.r.t. the current action-value function.\n",
    "\n",
    "    Important: assume you have a random number generator 'rand_generator' as a part of the class\n",
    "                which you can use as self.rand_generator.choice() or self.rand_generator.rand()\n",
    "\n",
    "    Args:\n",
    "        state (List): coordinates of the agent (two elements)\n",
    "    Returns:\n",
    "        The action taken w.r.t. the aforementioned epsilon-greedy policy\n",
    "    \"\"\"\n",
    "\n",
    "    if self.rand_generator.rand() < self.epsilon:\n",
    "        action = self.rand_generator.choice(self.actions)\n",
    "    else:\n",
    "        values = self.q_values[state]\n",
    "        action = self.argmax(values)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34c5fc71-bfa5-4969-8a12-4c443866b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DynaQAgent\n",
    "\n",
    "def agent_start(self, state):\n",
    "    \"\"\"The first method called when the experiment starts, \n",
    "    called after the environment starts.\n",
    "    Args:\n",
    "        state (Numpy array): the state from the\n",
    "            environment's env_start function.\n",
    "    Returns:\n",
    "        (int) the first action the agent takes.\n",
    "    \"\"\"\n",
    "    action = self.choose_action_egreedy(state)\n",
    "    \n",
    "    self.past_state = state\n",
    "    self.past_action = action\n",
    "    \n",
    "    return self.past_action\n",
    "\n",
    "def agent_step(self, reward, state):\n",
    "    \"\"\"A step taken by the agent.\n",
    "\n",
    "    Args:\n",
    "        reward (float): the reward received for taking the last action taken\n",
    "        state (Numpy array): the state from the\n",
    "            environment's step based on where the agent ended up after the\n",
    "            last step\n",
    "    Returns:\n",
    "        (int) The action the agent takes given this state.\n",
    "    \"\"\"\n",
    "\n",
    "    self.q_values[self.past_state][self.past_action] += self.step_size * (reward + self.gamma * np.max(self.q_values[state]) - self.q_values[self.past_state][self.past_action])\n",
    "    \n",
    "    self.update_model(self.past_state, self.past_action, state, reward)\n",
    "    \n",
    "    self.planning_step()\n",
    "    \n",
    "    action = self.choose_action_egreedy(state)\n",
    "    \n",
    "    self.past_state = state\n",
    "    self.past_action = action\n",
    "    \n",
    "    return self.past_action\n",
    "\n",
    "def agent_end(self, reward):\n",
    "    \"\"\"Called when the agent terminates.\n",
    "\n",
    "    Args:\n",
    "        reward (float): the reward the agent received for entering the\n",
    "            terminal state.\n",
    "    \"\"\"\n",
    "    \n",
    "    self.q_values[self.past_state][self.past_action] += self.step_size * (reward - self.q_values[self.past_state][self.past_action])\n",
    "    \n",
    "    self.update_model(self.past_state, self.past_action, -1, reward)\n",
    "    \n",
    "    self.planning_step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a41a709f-6d2d-47a4-a665-06140e7efc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1\n",
      "Model: \n",
      " {}\n",
      "Action-value estimates: \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 0}\n",
    "test_agent = DynaQAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "action = test_agent.agent_start(0)\n",
    "print(\"Action:\", action)\n",
    "print(\"Model: \\n\", test_agent.model)\n",
    "print(\"Action-value estimates: \\n\", test_agent.q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda11add-ff48-4019-a775-0ddd06f31560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: [1, 3, 1]\n",
      "Model: \n",
      " {0: {1: (2, 1)}, 2: {3: (1, 0)}}\n",
      "Action-value estimates: \n",
      " [[0.     0.3439 0.     0.    ]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"planning_steps\": 2,\n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 0}\n",
    "test_agent = DynaQAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "actions.append(test_agent.agent_start(0))\n",
    "actions.append(test_agent.agent_step(1,2))\n",
    "actions.append(test_agent.agent_step(0,1))\n",
    "print(\"Actions:\", actions)\n",
    "print(\"Model: \\n\", test_agent.model)\n",
    "print(\"Action-value estimates: \\n\", test_agent.q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7178c25-0669-4eb4-9ba2-a2829ec09dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: [1, 3, 1]\n",
      "Model: \n",
      " {0: {1: (2, 1)}, 2: {3: (1, 0)}, 1: {1: (-1, 1)}}\n",
      "Action-value Estimates: \n",
      " [[0.      0.41051 0.      0.     ]\n",
      " [0.      0.1     0.      0.     ]\n",
      " [0.      0.      0.      0.01   ]]\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"planning_steps\": 2,\n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 0}\n",
    "test_agent = DynaQAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "actions.append(test_agent.agent_start(0))\n",
    "actions.append(test_agent.agent_step(1,2))\n",
    "actions.append(test_agent.agent_step(0,1))\n",
    "test_agent.agent_end(1)\n",
    "print(\"Actions:\", actions)\n",
    "print(\"Model: \\n\", test_agent.model)\n",
    "print(\"Action-value Estimates: \\n\", test_agent.q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0e189-9cc9-4d0c-8c1d-f8b71e979ea3",
   "metadata": {},
   "source": [
    "The initial action values are 0, the step-size parameter is 0.125. and the exploration parameter is 0.1. After the experiment, the sum of rewards in each episode should match the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd199255-2d93-4820-9dc7-6ded3a4a31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, env_parameters, agent_parameters, exp_parameters):\n",
    "\n",
    "    # Experiment settings\n",
    "    num_runs = exp_parameters['num_runs']\n",
    "    num_episodes = exp_parameters['num_episodes']\n",
    "    planning_steps_all = agent_parameters['planning_steps']\n",
    "\n",
    "    env_info = env_parameters                     \n",
    "    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  # We pass the agent the information it needs. \n",
    "                  \"num_actions\" : agent_parameters[\"num_actions\"],\n",
    "                  \"epsilon\": agent_parameters[\"epsilon\"], \n",
    "                  \"discount\": env_parameters[\"discount\"],\n",
    "                  \"step_size\" : agent_parameters[\"step_size\"]}\n",
    "\n",
    "    all_averages = np.zeros((len(planning_steps_all), num_runs, num_episodes)) # for collecting metrics \n",
    "    log_data = {'planning_steps_all' : planning_steps_all}                     # that shall be plotted later\n",
    "\n",
    "    for idx, planning_steps in enumerate(planning_steps_all):\n",
    "\n",
    "        print('Planning steps : ', planning_steps)\n",
    "        os.system('sleep 0.5')                    # to prevent tqdm printing out-of-order before the above print()\n",
    "        agent_info[\"planning_steps\"] = planning_steps  \n",
    "\n",
    "        for i in tqdm(range(num_runs)):\n",
    "\n",
    "            agent_info['random_seed'] = i\n",
    "            agent_info['planning_random_seed'] = i\n",
    "\n",
    "            rl_glue = RLGlue(env, agent)          # Creates a new RLGlue experiment with the env and agent we chose above\n",
    "            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n",
    "\n",
    "            for j in range(num_episodes):\n",
    "\n",
    "                rl_glue.rl_start()                # We start an episode. Here we aren't using rl_glue.rl_episode()\n",
    "                                                  # like the other assessments because we'll be requiring some \n",
    "                is_terminal = False               # data from within the episodes in some of the experiments here \n",
    "                num_steps = 0\n",
    "                while not is_terminal:\n",
    "                    reward, _, action, is_terminal = rl_glue.rl_step()  # The environment and agent take a step \n",
    "                    num_steps += 1                                      # and return the reward and action taken.\n",
    "\n",
    "                all_averages[idx][i][j] = num_steps\n",
    "\n",
    "    log_data['all_averages'] = all_averages\n",
    "    np.save(\"results/Dyna-Q_planning_steps\", log_data)\n",
    "    \n",
    "\n",
    "def plot_steps_per_episode(file_path):\n",
    "\n",
    "    data = np.load(file_path).item()\n",
    "    all_averages = data['all_averages']\n",
    "    planning_steps_all = data['planning_steps_all']\n",
    "\n",
    "    for i, planning_steps in enumerate(planning_steps_all):\n",
    "        plt.plot(np.mean(all_averages[i], axis=0), label='Planning steps = '+str(planning_steps))\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Steps\\nper\\nepisode', rotation=0, labelpad=40)\n",
    "    plt.axhline(y=16, linestyle='--', color='grey', alpha=0.4)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97c4a9de-5030-4433-af40-a4181141ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_with_state_visitations(env, agent, env_parameters, agent_parameters, exp_parameters, result_file_name):\n",
    "\n",
    "    # Experiment settings\n",
    "    num_runs = exp_parameters['num_runs']\n",
    "    num_max_steps = exp_parameters['num_max_steps']\n",
    "    planning_steps_all = agent_parameters['planning_steps']\n",
    "\n",
    "    env_info = {\"change_at_n\" : env_parameters[\"change_at_n\"]}                     \n",
    "    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  \n",
    "                  \"num_actions\" : agent_parameters[\"num_actions\"],\n",
    "                  \"epsilon\": agent_parameters[\"epsilon\"], \n",
    "                  \"discount\": env_parameters[\"discount\"],\n",
    "                  \"step_size\" : agent_parameters[\"step_size\"]}\n",
    "\n",
    "    state_visits_before_change = np.zeros((len(planning_steps_all), num_runs, 54))  # For saving the number of\n",
    "    state_visits_after_change = np.zeros((len(planning_steps_all), num_runs, 54))   #     state-visitations \n",
    "    cum_reward_all = np.zeros((len(planning_steps_all), num_runs, num_max_steps))   # For saving the cumulative reward\n",
    "    log_data = {'planning_steps_all' : planning_steps_all}\n",
    "\n",
    "    for idx, planning_steps in enumerate(planning_steps_all):\n",
    "\n",
    "        print('Planning steps : ', planning_steps)\n",
    "        os.system('sleep 1')          # to prevent tqdm printing out-of-order before the above print()\n",
    "        agent_info[\"planning_steps\"] = planning_steps  # We pass the agent the information it needs. \n",
    "\n",
    "        for run in tqdm(range(num_runs)):\n",
    "\n",
    "            agent_info['random_seed'] = run\n",
    "            agent_info['planning_random_seed'] = run\n",
    "\n",
    "            rl_glue = RLGlue(env, agent)  # Creates a new RLGlue experiment with the env and agent we chose above\n",
    "            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n",
    "\n",
    "            num_steps = 0\n",
    "            cum_reward = 0\n",
    "\n",
    "            while num_steps < num_max_steps-1 :\n",
    "\n",
    "                state, _ = rl_glue.rl_start()  # We start the experiment. We'll be collecting the \n",
    "                is_terminal = False            # state-visitation counts to visiualize the learned policy\n",
    "                if num_steps < env_parameters[\"change_at_n\"]: \n",
    "                    state_visits_before_change[idx][run][state] += 1\n",
    "                else:\n",
    "                    state_visits_after_change[idx][run][state] += 1\n",
    "\n",
    "                while not is_terminal and num_steps < num_max_steps-1 :\n",
    "                    reward, state, action, is_terminal = rl_glue.rl_step()  \n",
    "                    num_steps += 1\n",
    "                    cum_reward += reward\n",
    "                    cum_reward_all[idx][run][num_steps] = cum_reward\n",
    "                    if num_steps < env_parameters[\"change_at_n\"]:\n",
    "                        state_visits_before_change[idx][run][state] += 1\n",
    "                    else:\n",
    "                        state_visits_after_change[idx][run][state] += 1\n",
    "\n",
    "    log_data['state_visits_before'] = state_visits_before_change\n",
    "    log_data['state_visits_after'] = state_visits_after_change\n",
    "    log_data['cum_reward_all'] = cum_reward_all\n",
    "    np.save(\"results/\" + result_file_name, log_data)\n",
    "\n",
    "def plot_cumulative_reward(file_path, item_key, y_key, y_axis_label, legend_prefix, title):\n",
    "    \n",
    "    data_all = np.load(file_path, allow_pickle=True).item() #TODO plotting issue with pickle \n",
    "    data_y_all = data_all[y_key]\n",
    "    items = data_all[item_key]\n",
    "\n",
    "    for i, item in enumerate(items):\n",
    "        plt.plot(np.mean(data_y_all[i], axis=0), label=legend_prefix+str(item))\n",
    "\n",
    "    plt.axvline(x=3000, linestyle='--', color='grey', alpha=0.4)\n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel(y_axis_label, rotation=0, labelpad=60)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1ccde48-513e-4504-a42d-305206fdd690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning steps :  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning steps :  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning steps :  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m current_env \u001b[38;5;241m=\u001b[39m ShortcutMazeEnvironment   \u001b[38;5;66;03m# The environment\u001b[39;00m\n\u001b[1;32m     22\u001b[0m current_agent \u001b[38;5;241m=\u001b[39m DynaQAgent              \u001b[38;5;66;03m# The agent\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mrun_experiment_with_state_visitations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDyna-Q_shortcut_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     25\u001b[0m plot_cumulative_reward(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/Dyna-Q_shortcut_steps.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplanning_steps_all\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcum_reward_all\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCumulative\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlanning steps = \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDyna-Q : Varying planning_steps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[36], line 47\u001b[0m, in \u001b[0;36mrun_experiment_with_state_visitations\u001b[0;34m(env, agent, env_parameters, agent_parameters, exp_parameters, result_file_name)\u001b[0m\n\u001b[1;32m     44\u001b[0m     state_visits_after_change[idx][run][state] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_terminal \u001b[38;5;129;01mand\u001b[39;00m num_steps \u001b[38;5;241m<\u001b[39m num_max_steps\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m :\n\u001b[0;32m---> 47\u001b[0m     reward, state, action, is_terminal \u001b[38;5;241m=\u001b[39m \u001b[43mrl_glue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     48\u001b[0m     num_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m     cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Desktop/Reinforcement-Learning/Dyna-Q/rl_glue.py:137\u001b[0m, in \u001b[0;36mRLGlue.rl_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     roat \u001b[38;5;241m=\u001b[39m (reward, last_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_action, term)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m roat\n",
      "File \u001b[0;32m<string>:35\u001b[0m, in \u001b[0;36magent_step\u001b[0;34m(self, reward, state)\u001b[0m\n",
      "File \u001b[0;32m<string>:15\u001b[0m, in \u001b[0;36mplanning_step\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2687\u001b[0m, in \u001b[0;36m_max_dispatcher\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ptp(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2684\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_ptp(a, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_max_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2688\u001b[0m                     where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[1;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,                     # The number of times we run the experiment\n",
    "    \"num_max_steps\" : 6000,              # The number of steps per experiment\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = { \n",
    "    \"discount\": 0.95,\n",
    "    \"change_at_n\": 3000\n",
    "}\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {  \n",
    "    \"num_states\" : 54,\n",
    "    \"num_actions\" : 4, \n",
    "    \"epsilon\": 0.1, \n",
    "    \"step_size\" : 0.125,\n",
    "    \"planning_steps\" : [5, 10, 50]      # The list of planning_steps we want to try\n",
    "}\n",
    "\n",
    "current_env = ShortcutMazeEnvironment   # The environment\n",
    "current_agent = DynaQAgent              # The agent\n",
    "\n",
    "run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, \"Dyna-Q_shortcut_steps\")    \n",
    "plot_cumulative_reward('results/Dyna-Q_shortcut_steps.npy', 'planning_steps_all', 'cum_reward_all', 'Cumulative\\nreward', 'Planning steps = ', 'Dyna-Q : Varying planning_steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02eded36-db3d-485a-b7e2-00e0b2a0c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_state_visitations(file_path, plot_titles, idx):\n",
    "\n",
    "    data = np.load(file_path).item()\n",
    "    data_keys = [\"state_visits_before\", \"state_visits_after\"]\n",
    "    positions = [211,212]\n",
    "    titles = plot_titles\n",
    "    wall_ends = [None,-1]\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        state_visits = data[data_keys[i]][idx]\n",
    "        average_state_visits = np.mean(state_visits, axis=0)\n",
    "        grid_state_visits = np.rot90(average_state_visits.reshape((6,9)).T)\n",
    "        grid_state_visits[2,1:wall_ends[i]] = np.nan # walls\n",
    "        #print(average_state_visits.reshape((6,9)))\n",
    "        plt.subplot(positions[i])\n",
    "        plt.pcolormesh(grid_state_visits, edgecolors='gray', linewidth=1, cmap='viridis')\n",
    "        plt.text(3+0.5, 0+0.5, 'S', horizontalalignment='center', verticalalignment='center')\n",
    "        plt.text(8+0.5, 5+0.5, 'G', horizontalalignment='center', verticalalignment='center')\n",
    "        plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "        cm = plt.get_cmap()\n",
    "        cm.set_bad('gray')\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.0, right=0.7, top=1.0)\n",
    "    cax = plt.axes([1., 0.0, 0.075, 1.])\n",
    "    cbar = plt.colorbar(cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7f61d-15d2-44ca-9ee9-847ce3791e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_state_visitations(\"results/Dyna-Q_shortcut_steps.npy\", ['Dyna-Q : State visitations before the env changes', 'Dyna-Q : State visitations after the env changes'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03efb2d7-24fd-4683-a43d-4556073f8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_only_cumulative_reward(env, agent, env_parameters, agent_parameters, exp_parameters):\n",
    "\n",
    "    # Experiment settings\n",
    "    num_runs = exp_parameters['num_runs']\n",
    "    num_max_steps = exp_parameters['num_max_steps']\n",
    "    epsilons = agent_parameters['epsilons']\n",
    "\n",
    "    env_info = {\"change_at_n\" : env_parameters[\"change_at_n\"]}                     \n",
    "    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  \n",
    "                  \"num_actions\" : agent_parameters[\"num_actions\"],\n",
    "                  \"planning_steps\": agent_parameters[\"planning_steps\"], \n",
    "                  \"discount\": env_parameters[\"discount\"],\n",
    "                  \"step_size\" : agent_parameters[\"step_size\"]}\n",
    "\n",
    "    log_data = {'epsilons' : epsilons} \n",
    "    cum_reward_all = np.zeros((len(epsilons), num_runs, num_max_steps))\n",
    "\n",
    "    for eps_idx, epsilon in enumerate(epsilons):\n",
    "\n",
    "        print('Agent : Dyna-Q, epsilon : %f' % epsilon)\n",
    "        os.system('sleep 1')          # to prevent tqdm printing out-of-order before the above print()\n",
    "        agent_info[\"epsilon\"] = epsilon\n",
    "\n",
    "        for run in tqdm(range(num_runs)):\n",
    "\n",
    "            agent_info['random_seed'] = run\n",
    "            agent_info['planning_random_seed'] = run\n",
    "\n",
    "            rl_glue = RLGlue(env, agent)  # Creates a new RLGlue experiment with the env and agent we chose above\n",
    "            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n",
    "\n",
    "            num_steps = 0\n",
    "            cum_reward = 0\n",
    "\n",
    "            while num_steps < num_max_steps-1 :\n",
    "\n",
    "                rl_glue.rl_start()  # We start the experiment\n",
    "                is_terminal = False\n",
    "\n",
    "                while not is_terminal and num_steps < num_max_steps-1 :\n",
    "                    reward, _, action, is_terminal = rl_glue.rl_step()  # The environment and agent take a step and return\n",
    "                    # the reward, and action taken.\n",
    "                    num_steps += 1\n",
    "                    cum_reward += reward\n",
    "                    cum_reward_all[eps_idx][run][num_steps] = cum_reward\n",
    "\n",
    "    log_data['cum_reward_all'] = cum_reward_all\n",
    "    np.save(\"results/Dyna-Q_epsilons\", log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8ed4a18-f6d3-4cde-9bdb-c81a7201e40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent : Dyna-Q, epsilon : 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [00:27<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent : Dyna-Q, epsilon : 0.200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████                       | 14/30 [00:13<00:15,  1.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m current_env \u001b[38;5;241m=\u001b[39m ShortcutMazeEnvironment   \u001b[38;5;66;03m# The environment\u001b[39;00m\n\u001b[1;32m     22\u001b[0m current_agent \u001b[38;5;241m=\u001b[39m DynaQAgent              \u001b[38;5;66;03m# The agent\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mrun_experiment_only_cumulative_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m plot_cumulative_reward(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/Dyna-Q_epsilons.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilons\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcum_reward_all\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCumulative\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mepsilon$ = \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDyna-Q : Varying $\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mepsilon$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[40], line 41\u001b[0m, in \u001b[0;36mrun_experiment_only_cumulative_reward\u001b[0;34m(env, agent, env_parameters, agent_parameters, exp_parameters)\u001b[0m\n\u001b[1;32m     38\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_terminal \u001b[38;5;129;01mand\u001b[39;00m num_steps \u001b[38;5;241m<\u001b[39m num_max_steps\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m :\n\u001b[0;32m---> 41\u001b[0m     reward, _, action, is_terminal \u001b[38;5;241m=\u001b[39m \u001b[43mrl_glue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# The environment and agent take a step and return\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# the reward, and action taken.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     num_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Reinforcement-Learning/Dyna-Q/rl_glue.py:137\u001b[0m, in \u001b[0;36mRLGlue.rl_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     roat \u001b[38;5;241m=\u001b[39m (reward, last_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_action, term)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m roat\n",
      "File \u001b[0;32m<string>:31\u001b[0m, in \u001b[0;36magent_step\u001b[0;34m(self, reward, state)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[1;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2698\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_parameters = {\n",
    "    \"num_runs\" : 30,                     # The number of times we run the experiment\n",
    "    \"num_max_steps\" : 6000,              # The number of steps per experiment\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = { \n",
    "    \"discount\": 0.95,\n",
    "    \"change_at_n\": 3000\n",
    "}\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {  \n",
    "    \"num_states\" : 54,\n",
    "    \"num_actions\" : 4, \n",
    "    \"step_size\" : 0.125,\n",
    "    \"planning_steps\" : 10,\n",
    "    \"epsilons\": [0.1, 0.2, 0.4, 0.8]    # The list of epsilons we want to try\n",
    "}\n",
    "\n",
    "current_env = ShortcutMazeEnvironment   # The environment\n",
    "current_agent = DynaQAgent              # The agent\n",
    "\n",
    "run_experiment_only_cumulative_reward(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n",
    "plot_cumulative_reward('results/Dyna-Q_epsilons.npy', 'epsilons', 'cum_reward_all', 'Cumulative\\nreward', r'$\\epsilon$ = ', r'Dyna-Q : Varying $\\epsilon$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a52c1-b1c8-4a3b-99e8-080d3d123d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
