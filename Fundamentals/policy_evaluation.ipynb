{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fca7e5d-2ce1-45b8-97ca-d09ead1bf543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rlglue.rl_glue import RLGlue\n",
    "\n",
    "from rlglue.agent import BaseAgent\n",
    "from rlglue.environment import BaseEnvironment \n",
    "\n",
    "import Manager # visualization \n",
    "from itertools import product\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b6ab48-32ff-424f-8d89-a857466e9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkEnvironment(BaseEnvironment):\n",
    "    def env_init(self, agent_info={}):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_start(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_step(self, reward, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_end(self, reward):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def env_cleanup(self, reward):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # helper method\n",
    "    def state(self, loc):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8b8a799-0b8d-4997-92f2-5cc9fbf480f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CliffWalkEnvironment\n",
    "\n",
    "def env_init(self, env_info={}):\n",
    "        \n",
    "        # Note, we can setup the following variables later, in env_start() as it is equivalent. \n",
    "        # Code is left here to adhere to the note above, but these variables are initialized once more\n",
    "        # in env_start() [See the env_start() function below.]\n",
    "        \n",
    "        reward = None\n",
    "        state = None # See Aside\n",
    "        termination = None\n",
    "        self.reward_state_term = (reward, state, termination)\n",
    "        \n",
    "        # AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably \n",
    "        # used with the term \"state\" for our purposes and for this assignment in particular. \n",
    "        # A difference arises in the use of the terms when we have what is called Partial Observability where \n",
    "        # the environment may return states that may not fully represent all the information needed to \n",
    "        # predict values or make decisions (i.e., the environment is non-Markovian.)\n",
    "        \n",
    "        # Set the default height to 4 and width to 12 (as in the diagram given above)\n",
    "        self.grid_h = env_info.get(\"grid_height\", 4) \n",
    "        self.grid_w = env_info.get(\"grid_width\", 12)\n",
    "        \n",
    "        # Now, we can define a frame of reference. Let positive x be towards the direction down and \n",
    "        # positive y be towards the direction right (following the row-major NumPy convention.)\n",
    "        # Then, keeping with the usual convention that arrays are 0-indexed, max x is then grid_h - 1 \n",
    "        # and max y is then grid_w - 1. So, we have:\n",
    "        # Starting location of agent is the bottom-left corner, (max x, min y). \n",
    "        self.start_loc = (self.grid_h - 1, 0)\n",
    "        # Goal location is the bottom-right corner. (max x, max y).\n",
    "        self.goal_loc = (self.grid_h - 1, self.grid_w - 1)\n",
    "        \n",
    "        # The cliff will contain all the cells between the start_loc and goal_loc.\n",
    "        self.cliff = [(self.grid_h - 1, i) for i in range(1, (self.grid_w - 1))]\n",
    "        \n",
    "        # Take a look at the annotated environment diagram given in the above Jupyter Notebook cell to \n",
    "        # verify that your understanding of the above code is correct for the default case, i.e., where \n",
    "        # height = 4 and width = 12.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc6f6836-79d3-42a1-bda3-0930cbba8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CliffWalkEnvironment\n",
    "\n",
    "def state(self, loc):\n",
    "    ### START CODE HERE ###\n",
    "    i, j = loc\n",
    "    return i * self.grid_w + j\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dc17819-73cc-4e72-aced-17ed27e33a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_state():\n",
    "    env = CliffWalkEnvironment()\n",
    "    env.env_init({\"grid_height\": 4, \"grid_width\": 12})\n",
    "    coords_to_test = [(0, 0), (0, 11), (1, 5), (3, 0), (3, 9), (3, 11)]\n",
    "    true_states = [0, 11, 17, 36, 45, 47]\n",
    "    output_states = [env.state(coords) for coords in coords_to_test]\n",
    "    assert(output_states == true_states)\n",
    "test_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a080e31-03cd-444c-98c4-5e7148a82fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CliffWalkEnvironment\n",
    "\n",
    "def env_start(self):\n",
    "    \n",
    "    reward = 0\n",
    "    # agent_loc will hold the current location of the agent\n",
    "    self.agent_loc = self.start_loc\n",
    "    # state is the one dimensional state representation of the agent location.\n",
    "    state = self.state(self.agent_loc)\n",
    "    termination = False\n",
    "    self.reward_state_term = (reward, state, termination)\n",
    "\n",
    "    return self.reward_state_term[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9983be6-25e2-4f42-92cf-7bea9157fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CliffWalkEnvironment\n",
    "\n",
    "def env_step(self, action):\n",
    "\n",
    "    if action == 0: # UP \n",
    "        possible_next_loc = (self.agent_loc[0] - 1, self.agent_loc[1])\n",
    "        if possible_next_loc[0] >= 0: # Within Bounds?\n",
    "            self.agent_loc = possible_next_loc\n",
    "        else:\n",
    "            pass # Stay.\n",
    "    elif action == 1: # LEFT\n",
    "        possible_next_loc = (self.agent_loc[0], self.agent_loc[1] - 1)\n",
    "        if possible_next_loc[1] >= 0: # Within Bounds?\n",
    "            self.agent_loc = possible_next_loc\n",
    "        else:\n",
    "            pass # Stay.\n",
    "    elif action == 2: # DOWN\n",
    "        possible_next_loc = (self.agent_loc[0] + 1, self.agent_loc[1])\n",
    "        if possible_next_loc[0] < self.grid_h: # Within Bounds?\n",
    "            self.agent_loc = possible_next_loc\n",
    "        else:\n",
    "            pass # Stay.\n",
    "    elif action == 3: # RIGHT\n",
    "        possible_next_loc = (self.agent_loc[0], self.agent_loc[1] + 1)\n",
    "        if possible_next_loc[1] < self.grid_w: # Within Bounds?\n",
    "            self.agent_loc = possible_next_loc\n",
    "        else:\n",
    "            pass # Stay.\n",
    "    else: \n",
    "        raise Exception(str(action) + \" not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!\")\n",
    "\n",
    "    reward = -1\n",
    "    terminal = False\n",
    "\n",
    "    if self.agent_loc == self.goal_loc: # Reached Goal!\n",
    "        reward = -1\n",
    "        terminal = True\n",
    "    elif self.agent_loc in self.cliff: # Fell into the cliff!\n",
    "        reward = -100\n",
    "        terminal = False\n",
    "        self.agent_loc = self.start_loc\n",
    "    else: \n",
    "        reward = -1\n",
    "        terminal = False\n",
    "\n",
    "    self.reward_state_term = (reward, self.state(self.agent_loc), terminal)\n",
    "    return self.reward_state_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c21baf7-9fdf-4790-b642-9d04fbd776dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_action_up():\n",
    "    env = CliffWalkEnvironment()\n",
    "    env.env_init({\"grid_height\": 4, \"grid_width\": 12})\n",
    "    env.agent_loc = (0, 0)\n",
    "    env.env_step(0)\n",
    "    assert(env.agent_loc == (0, 0))\n",
    "    \n",
    "    env.agent_loc = (1, 0)\n",
    "    env.env_step(0)\n",
    "    assert(env.agent_loc == (0, 0))\n",
    "test_action_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a7427d6-c7ab-41cd-884b-9436cb018549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward():\n",
    "    env = CliffWalkEnvironment()\n",
    "    env.env_init({\"grid_height\": 4, \"grid_width\": 12})\n",
    "    env.agent_loc = (0, 0)\n",
    "    reward_state_term = env.env_step(0)\n",
    "    assert(reward_state_term[0] == -1 and reward_state_term[1] == env.state((0, 0)) and\n",
    "           reward_state_term[2] == False)\n",
    "    \n",
    "    env.agent_loc = (3, 1)\n",
    "    reward_state_term = env.env_step(2)\n",
    "    assert(reward_state_term[0] == -100 and reward_state_term[1] == env.state((3, 0)) and\n",
    "           reward_state_term[2] == False)\n",
    "    \n",
    "    env.agent_loc = (2, 11)\n",
    "    reward_state_term = env.env_step(2)\n",
    "    assert(reward_state_term[0] == -1 and reward_state_term[1] == env.state((3, 11)) and\n",
    "           reward_state_term[2] == True)\n",
    "test_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c70ed213-8e91-49ac-92df-d44d276a801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CliffWalkEnvironment\n",
    "\n",
    "def env_cleanup(self):\n",
    "    \"\"\"Cleanup done after the environment ends\"\"\"\n",
    "    self.agent_loc = self.start_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d4162e9-2c8b-4c18-a072-19bf6f44153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent(BaseAgent):\n",
    "    def agent_init(self, agent_info={}):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def agent_cleanup(self):        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04190d20-8ba9-4d8e-b49f-0fe91922c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to TDAgent\n",
    "\n",
    "def agent_init(self, agent_info={}):\n",
    "    \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "\n",
    "    # Create a random number generator with the provided seed to seed the agent for reproducibility.\n",
    "    self.rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n",
    "\n",
    "    # Policy will be given, recall that the goal is to accurately estimate its corresponding value function. \n",
    "    self.policy = agent_info.get(\"policy\")\n",
    "    # Discount factor (gamma) to use in the updates.\n",
    "    self.discount = agent_info.get(\"discount\")\n",
    "    # The learning rate or step size parameter (alpha) to use in updates.\n",
    "    self.step_size = agent_info.get(\"step_size\")\n",
    "\n",
    "    # Initialize an array of zeros that will hold the values.\n",
    "    # Recall that the policy can be represented as a (# States, # Actions) array. With the \n",
    "    # assumption that this is the case, we can use the first dimension of the policy to\n",
    "    # initialize the array for values.\n",
    "    self.values = np.zeros((self.policy.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "569f9540-38a3-4d7f-aa24-4cef3f4c2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to TDAgent\n",
    "\n",
    "def agent_start(self, state):\n",
    "    \"\"\"The first method called when the episode starts, called after\n",
    "    the environment starts.\n",
    "    Args:\n",
    "        state (Numpy array): the state from the environment's env_start function.\n",
    "    Returns:\n",
    "        The first action the agent takes.\n",
    "    \"\"\"\n",
    "    # The policy can be represented as a (# States, # Actions) array. So, we can use \n",
    "    # the second dimension here when choosing an action.\n",
    "    action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n",
    "    self.last_state = state\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bb26a17-3f67-4733-b304-332ab256c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to TDAgent\n",
    "\n",
    "def agent_step(self, reward, state):\n",
    "    \"\"\"A step taken by the agent.\n",
    "    Args:\n",
    "        reward (float): the reward received for taking the last action taken\n",
    "        state (Numpy array): the state from the\n",
    "            environment's step after the last action, i.e., where the agent ended up after the\n",
    "            last action\n",
    "    Returns:\n",
    "        The action the agent is taking.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # Hint: We should perform an update with the last state given that we now have the reward and\n",
    "    # next state. We break this into two steps. Recall for example that the Monte-Carlo update \n",
    "    # had the form: V[S_t] = V[S_t] + alpha * (target - V[S_t]), where the target was the return, G_t.\n",
    "    target = reward + self.discount * self.values[state]\n",
    "    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Having updated the value for the last state, we now act based on the current \n",
    "    # state, and set the last state to be current one as we will next be making an \n",
    "    # update with it when agent_step is called next once the action we return from this function \n",
    "    # is executed in the environment.\n",
    "\n",
    "    action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n",
    "    self.last_state = state\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91ad52e3-d3a7-418a-ab9f-f169488d3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to TDAgent\n",
    "\n",
    "def agent_end(self, reward):\n",
    "    \"\"\"Run when the agent terminates.\n",
    "    Args:\n",
    "        reward (float): the reward the agent received for entering the terminal state.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # Hint: Here too, we should perform an update with the last state given that we now have the \n",
    "    # reward. Note that in this case, the action led to termination. Once more, we break this into \n",
    "    # two steps, computing the target and the update itself that uses the target and the \n",
    "    # current value estimate for the state whose value we are updating.\n",
    "    target = reward\n",
    "    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46f9ba78-a3e9-4be0-9086-22fb01b51959",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to TDAgent\n",
    "\n",
    "def agent_cleanup(self):\n",
    "    \"\"\"Cleanup done after the agent ends.\"\"\"\n",
    "    self.last_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e7cd787-bf10-44f2-851b-2661998e24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to TDAgent\n",
    "\n",
    "def agent_message(self, message):\n",
    "    \"\"\"A function used to pass information from the agent to the experiment.\n",
    "    Args:\n",
    "        message: The message passed to the agent.\n",
    "    Returns:\n",
    "        The response (or answer) to the message.\n",
    "    \"\"\"\n",
    "    if message == \"get_values\":\n",
    "        return self.values\n",
    "    else:\n",
    "        raise Exception(\"TDAgent.agent_message(): Message not understood!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f7fe48b-b4fe-4776-8dfe-5c48773fb1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to TDAgent\n",
    "\n",
    "\n",
    "def agent_message(self, message):\n",
    "    \"\"\"A function used to pass information from the agent to the experiment.\n",
    "    Args:\n",
    "        message: The message passed to the agent.\n",
    "    Returns:\n",
    "        The response (or answer) to the message.\n",
    "    \"\"\"\n",
    "    if message == \"get_values\":\n",
    "        return self.values\n",
    "    else:\n",
    "        raise Exception(\"TDAgent.agent_message(): Message not understood!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b0c9afd-1fa3-41f4-90f9-80c181755a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    " \n",
    "def run_experiment(env_info, agent_info, \n",
    "                   num_episodes=5000,\n",
    "                   experiment_name=None,\n",
    "                   plot_freq=100,\n",
    "                   true_values_file=None,\n",
    "                   value_error_threshold=1e-8):\n",
    "    env = CliffWalkEnvironment\n",
    "    agent = TDAgent\n",
    "    rl_glue = RLGlue(env, agent)\n",
    "\n",
    "    rl_glue.rl_init(agent_info, env_info)\n",
    "\n",
    "    manager = Manager(env_info, agent_info, true_values_file=true_values_file, experiment_name=experiment_name)\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        rl_glue.rl_episode(0) # no step limit\n",
    "        if episode % plot_freq == 0:\n",
    "            values = rl_glue.agent.agent_message(\"get_values\")\n",
    "            manager.visualize(values, episode)\n",
    "\n",
    "    values = rl_glue.agent.agent_message(\"get_values\")\n",
    "    if true_values_file is not None:\n",
    "        # Grading: The Manager will check that the values computed using your TD agent match \n",
    "        # the true values (within some small allowance) across the states. In addition, it also\n",
    "        # checks whether the root mean squared value error is close to 0.\n",
    "        manager.run_tests(values, value_error_threshold)\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c71d1-127d-4ccc-8661-412d18490620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def run_experiment(env_info, agent_info, \n",
    "                   num_episodes=5000,\n",
    "                   experiment_name=None,\n",
    "                   plot_freq=100,\n",
    "                   true_values_file=None,\n",
    "                   value_error_threshold=1e-8):\n",
    "    env = CliffWalkEnvironment\n",
    "    agent = TDAgent\n",
    "    rl_glue = RLGlue(env, agent)\n",
    "\n",
    "    rl_glue.rl_init(agent_info, env_info)\n",
    "\n",
    "    manager = Manager(env_info, agent_info, true_values_file=true_values_file, experiment_name=experiment_name)\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        rl_glue.rl_episode(0) # no step limit\n",
    "        if episode % plot_freq == 0:\n",
    "            values = rl_glue.agent.agent_message(\"get_values\")\n",
    "            manager.visualize(values, episode)\n",
    "\n",
    "    values = rl_glue.agent.agent_message(\"get_values\")\n",
    "    if true_values_file is not None:\n",
    "        # Grading: The Manager will check that the values computed using your TD agent match \n",
    "        # the true values (within some small allowance) across the states. In addition, it also\n",
    "        # checks whether the root mean squared value error is close to 0.\n",
    "        manager.run_tests(values, value_error_threshold)\n",
    "    \n",
    "    return values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
